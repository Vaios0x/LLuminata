/**
 * Sistema de An√°lisis Visual Avanzado para Engagement Educativo
 * Combina Computer Vision, Deep Learning y Neurociencia
 * para monitoreo de engagement en tiempo real
 */

import * as tf from '@tensorflow/tfjs';
import '@tensorflow/tfjs-backend-webgl';
import * as faceapi from 'face-api.js';

// Tipos para an√°lisis visual
export interface EngagementMetrics {
  userId: string;
  timestamp: Date;
  overallEngagement: number; // 0-1
  attentionLevel: number; // 0-1
  emotionalState: EmotionalState;
  cognitiveLoad: number; // 0-1
  fatigueLevel: number; // 0-1
  distractionEvents: DistractionEvent[];
  gazePath: GazePoint[];
  posturalAnalysis: PosturalMetrics;
  microExpressions: MicroExpression[];
  blinkPattern: BlinkAnalysis;
  neurophysiologicalIndicators: NeurophysiologicalMetrics;
  culturalContext: CulturalEngagementContext;
}

export interface EmotionalState {
  primary: EmotionType;
  secondary?: EmotionType;
  intensity: number; // 0-1
  valence: number; // -1 to 1 (negative to positive)
  arousal: number; // 0-1 (calm to excited)
  confidence: number; // 0-1
  culturallyAdjusted: boolean;
}

export type EmotionType = 
  | 'neutral' | 'happy' | 'sad' | 'angry' | 'fearful' | 'disgusted' | 'surprised'
  | 'confused' | 'frustrated' | 'concentrated' | 'interested' | 'bored' | 'excited';

export interface DistractionEvent {
  timestamp: Date;
  type: 'gaze_away' | 'posture_change' | 'movement' | 'device_interaction' | 'external_stimulus';
  duration: number; // milliseconds
  severity: 'low' | 'medium' | 'high';
  impact: number; // 0-1
  recovery_time: number; // milliseconds
}

export interface GazePoint {
  x: number; // Screen coordinates
  y: number;
  timestamp: number;
  confidence: number;
  fixationDuration?: number; // If part of a fixation
  saccadeSpeed?: number; // If transitioning
}

export interface PosturalMetrics {
  headPose: {
    pitch: number; // -90 to 90 degrees
    yaw: number; // -90 to 90 degrees  
    roll: number; // -90 to 90 degrees
    stability: number; // 0-1
  };
  shoulderAlignment: number; // 0-1 (0 = poor, 1 = excellent)
  spinalPosture: number; // 0-1
  leaningDirection: 'forward' | 'backward' | 'left' | 'right' | 'neutral';
  postureChanges: number; // Count of significant posture changes
  ergonomicScore: number; // 0-1
}

export interface MicroExpression {
  type: EmotionType;
  intensity: number;
  duration: number; // milliseconds
  timestamp: Date;
  confidence: number;
  facialRegion: 'eyes' | 'mouth' | 'brows' | 'full_face';
  suppressedEmotion?: EmotionType; // What the person might be hiding
}

export interface BlinkAnalysis {
  averageRate: number; // blinks per minute
  variability: number; // standard deviation
  microSleepEvents: number; // Extended blinks indicating fatigue
  concentrationBlinks: number; // Reduced blinking during focus
  stressIndicators: number; // Rapid or irregular blinking
}

export interface NeurophysiologicalMetrics {
  cognitiveLoadIndicators: {
    pupilDilation: number; // 0-1
    blinkRate: number;
    microsaccadeRate: number;
    fixationStability: number;
  };
  attentionalState: {
    sustainedAttention: number; // 0-1
    selectiveAttention: number; // 0-1
    dividedAttention: number; // 0-1
    attentionalFlexibility: number; // 0-1
  };
  executiveFunctionIndicators: {
    inhibitoryControl: number; // 0-1
    workingMemoryLoad: number; // 0-1
    cognitiveFlexibility: number; // 0-1
    planningAndOrganization: number; // 0-1
  };
  memoryProcessing: {
    encodingEfficiency: number; // 0-1
    retrievalSuccess: number; // 0-1
    consolidationQuality: number; // 0-1
  };
}

export interface CulturalEngagementContext {
  culturalBackground: string;
  nonverbalCommunicationStyle: 'direct' | 'indirect' | 'contextual';
  eyeContactNorms: 'high' | 'moderate' | 'low' | 'variable';
  expressivenessCultural: number; // 0-1
  collectivistVsIndividualist: number; // 0-1 (0=collectivist, 1=individualist)
  powerDistanceOrientation: number; // 0-1
  uncertaintyAvoidance: number; // 0-1
  culturalBiasAdjustment: number; // Applied correction factor
}

export interface AttentionHeatmap {
  width: number;
  height: number;
  data: number[][]; // 2D array of attention weights
  timestamp: Date;
  duration: number;
  screenElements: AttentionRegion[];
}

export interface AttentionRegion {
  element: string; // Element identifier
  x: number;
  y: number;
  width: number;
  height: number;
  totalAttention: number; // 0-1
  averageFixationDuration: number;
  revisitCount: number;
  importance: number; // Learning importance of this region
}

/**
 * Motor de An√°lisis Visual con Computer Vision Avanzado
 */
export class VisionAnalyticsService {
  private faceDetectionModel: any = null;
  private emotionModel: tf.LayersModel | null = null;
  private gazeTrackingModel: tf.LayersModel | null = null;
  private engagementModel: tf.LayersModel | null = null;
  private postureModel: tf.LayersModel | null = null;
  private microExpressionModel: tf.LayersModel | null = null;
  private cognitiveLoadModel: tf.LayersModel | null = null;
  
  private isInitialized = false;
  private isTracking = false;
  private videoElement: HTMLVideoElement | null = null;
  private canvasElement: HTMLCanvasElement | null = null;
  private stream: MediaStream | null = null;
  
  private currentMetrics: EngagementMetrics | null = null;
  private baselineMetrics: EngagementMetrics | null = null;
  private metricsHistory: EngagementMetrics[] = [];
  
  private culturalCalibration = new Map<string, any>();
  private personalCalibration: any = null;
  
  // Configuraci√≥n y umbrales
  private config = {
    frameRate: 30, // FPS for analysis
    analysisInterval: 100, // ms between analyses
    calibrationDuration: 30000, // 30 seconds for baseline
    attentionThreshold: 0.7,
    distractionThreshold: 0.3,
    fatigueThreshold: 0.6,
    emotionConfidenceThreshold: 0.5,
    gazeAccuracyThreshold: 0.8,
    privacyMode: true, // Process locally, don't store raw video
    culturalSensitivity: true
  };

  constructor() {
    this.initializeVisionModels();
  }

  /**
   * Inicializar todos los modelos de visi√≥n computacional
   */
  private async initializeVisionModels(): Promise<void> {
    try {
      console.log('üöÄ Inicializando sistema de an√°lisis visual...');
      
      await Promise.all([
        this.initializeFaceDetection(),
        this.initializeEmotionRecognition(),
        this.initializeGazeTracking(),
        this.initializeEngagementModel(),
        this.initializePostureAnalysis(),
        this.initializeMicroExpressionDetection(),
        this.initializeCognitiveLoadModel()
      ]);
      
      // Cargar calibraciones culturales
      await this.loadCulturalCalibrations();
      
      this.isInitialized = true;
      console.log('‚úÖ Sistema de an√°lisis visual inicializado');
    } catch (error) {
      console.error('‚ùå Error inicializando an√°lisis visual:', error);
      throw error;
    }
  }

  /**
   * Inicializar detecci√≥n facial con face-api.js
   */
  private async initializeFaceDetection(): Promise<void> {
    try {
      // Cargar modelos de face-api.js
      await Promise.all([
        faceapi.nets.tinyFaceDetector.loadFromUri('/models/face-api'),
        faceapi.nets.faceLandmark68Net.loadFromUri('/models/face-api'),
        faceapi.nets.faceRecognitionNet.loadFromUri('/models/face-api'),
        faceapi.nets.faceExpressionNet.loadFromUri('/models/face-api'),
        faceapi.nets.ageGenderNet.loadFromUri('/models/face-api')
      ]);
      
      this.faceDetectionModel = faceapi;
      console.log('‚úÖ Modelos de detecci√≥n facial cargados');
    } catch (error) {
      console.error('‚ö†Ô∏è Error cargando face-api, usando modelo alternativo:', error);
      await this.createAlternativeFaceModel();
    }
  }

  /**
   * Crear modelo alternativo de detecci√≥n facial
   */
  private async createAlternativeFaceModel(): Promise<void> {
    // Modelo CNN simple para detecci√≥n facial como fallback
    const model = tf.sequential({
      layers: [
        tf.layers.conv2d({
          filters: 32,
          kernelSize: 3,
          activation: 'relu',
          inputShape: [224, 224, 3]
        }),
        tf.layers.maxPooling2d({ poolSize: 2 }),
        tf.layers.conv2d({ filters: 64, kernelSize: 3, activation: 'relu' }),
        tf.layers.maxPooling2d({ poolSize: 2 }),
        tf.layers.conv2d({ filters: 128, kernelSize: 3, activation: 'relu' }),
        tf.layers.globalAveragePooling2d(),
        tf.layers.dense({ units: 256, activation: 'relu' }),
        tf.layers.dropout({ rate: 0.5 }),
        tf.layers.dense({ units: 5, activation: 'softmax' }) // face regions
      ]
    });
    
    model.compile({
      optimizer: 'adam',
      loss: 'categoricalCrossentropy',
      metrics: ['accuracy']
    });
    
    this.faceDetectionModel = model;
    console.log('‚úÖ Modelo alternativo de detecci√≥n facial creado');
  }

  /**
   * Inicializar reconocimiento de emociones
   */
  private async initializeEmotionRecognition(): Promise<void> {
    try {
      this.emotionModel = await tf.loadLayersModel('/models/emotion-recognition.json');
      console.log('‚úÖ Modelo de reconocimiento emocional cargado');
    } catch (error) {
      console.log('‚ö†Ô∏è Creando modelo de reconocimiento emocional...');
      
      // Modelo CNN + LSTM para reconocimiento emocional temporal
      const faceInput = tf.input({ shape: [64, 64, 3], name: 'face_region' });
      const eyeInput = tf.input({ shape: [32, 64, 3], name: 'eye_region' });
      const mouthInput = tf.input({ shape: [32, 48, 3], name: 'mouth_region' });
      const temporalInput = tf.input({ shape: [10, 13], name: 'emotion_history' }); // 10 frames, 13 emotions
      
      // Procesar regi√≥n facial completa
      let faceFeatures = tf.layers.conv2d({
        filters: 64,
        kernelSize: 5,
        activation: 'relu',
        name: 'face_conv1'
      }).apply(faceInput) as tf.SymbolicTensor;
      
      faceFeatures = tf.layers.maxPooling2d({ poolSize: 2 }).apply(faceFeatures) as tf.SymbolicTensor;
      
      faceFeatures = tf.layers.conv2d({
        filters: 128,
        kernelSize: 3,
        activation: 'relu',
        name: 'face_conv2'
      }).apply(faceFeatures) as tf.SymbolicTensor;
      
      faceFeatures = tf.layers.globalAveragePooling2d({ name: 'face_pool' }).apply(faceFeatures) as tf.SymbolicTensor;
      
      // Procesar regi√≥n de ojos
      let eyeFeatures = tf.layers.conv2d({
        filters: 32,
        kernelSize: 3,
        activation: 'relu',
        name: 'eye_conv1'
      }).apply(eyeInput) as tf.SymbolicTensor;
      
      eyeFeatures = tf.layers.conv2d({
        filters: 64,
        kernelSize: 3,
        activation: 'relu',
        name: 'eye_conv2'
      }).apply(eyeFeatures) as tf.SymbolicTensor;
      
      eyeFeatures = tf.layers.globalAveragePooling2d({ name: 'eye_pool' }).apply(eyeFeatures) as tf.SymbolicTensor;
      
      // Procesar regi√≥n de boca
      let mouthFeatures = tf.layers.conv2d({
        filters: 32,
        kernelSize: 3,
        activation: 'relu',
        name: 'mouth_conv1'
      }).apply(mouthInput) as tf.SymbolicTensor;
      
      mouthFeatures = tf.layers.conv2d({
        filters: 64,
        kernelSize: 3,
        activation: 'relu',
        name: 'mouth_conv2'
      }).apply(mouthFeatures) as tf.SymbolicTensor;
      
      mouthFeatures = tf.layers.globalAveragePooling2d({ name: 'mouth_pool' }).apply(mouthFeatures) as tf.SymbolicTensor;
      
      // Procesar historia temporal
      let temporalFeatures = tf.layers.lstm({
        units: 64,
        returnSequences: false,
        name: 'temporal_lstm'
      }).apply(temporalInput) as tf.SymbolicTensor;
      
      // Combinar todas las caracter√≠sticas
      const combinedFeatures = tf.layers.concatenate({
        name: 'emotion_feature_fusion'
      }).apply([faceFeatures, eyeFeatures, mouthFeatures, temporalFeatures]) as tf.SymbolicTensor;
      
      // Red de clasificaci√≥n emocional
      let emotionHidden = tf.layers.dense({
        units: 256,
        activation: 'relu',
        name: 'emotion_hidden1'
      }).apply(combinedFeatures) as tf.SymbolicTensor;
      
      emotionHidden = tf.layers.dropout({ rate: 0.4 }).apply(emotionHidden) as tf.SymbolicTensor;
      
      emotionHidden = tf.layers.dense({
        units: 128,
        activation: 'relu',
        name: 'emotion_hidden2'
      }).apply(emotionHidden) as tf.SymbolicTensor;
      
      // M√∫ltiples outputs emocionales
      const basicEmotions = tf.layers.dense({
        units: 7, // 7 emociones b√°sicas
        activation: 'softmax',
        name: 'basic_emotions'
      }).apply(emotionHidden) as tf.SymbolicTensor;
      
      const complexEmotions = tf.layers.dense({
        units: 6, // Emociones complejas (confusi√≥n, frustraci√≥n, etc.)
        activation: 'softmax',
        name: 'complex_emotions'
      }).apply(emotionHidden) as tf.SymbolicTensor;
      
      const valenceArousal = tf.layers.dense({
        units: 2, // Valencia y activaci√≥n
        activation: 'tanh',
        name: 'valence_arousal'
      }).apply(emotionHidden) as tf.SymbolicTensor;
      
      const emotionIntensity = tf.layers.dense({
        units: 1,
        activation: 'sigmoid',
        name: 'emotion_intensity'
      }).apply(emotionHidden) as tf.SymbolicTensor;
      
      this.emotionModel = tf.model({
        inputs: [faceInput, eyeInput, mouthInput, temporalInput],
        outputs: [basicEmotions, complexEmotions, valenceArousal, emotionIntensity],
        name: 'emotion_recognition_model'
      });
      
      this.emotionModel.compile({
        optimizer: tf.train.adam(0.001),
        loss: {
          'basic_emotions': 'categoricalCrossentropy',
          'complex_emotions': 'categoricalCrossentropy',
          'valence_arousal': 'meanSquaredError',
          'emotion_intensity': 'meanSquaredError'
        },
        metrics: ['accuracy']
      });
      
      console.log('‚úÖ Modelo de reconocimiento emocional creado');
    }
  }

  /**
   * Inicializar seguimiento de mirada (gaze tracking)
   */
  private async initializeGazeTracking(): Promise<void> {
    try {
      this.gazeTrackingModel = await tf.loadLayersModel('/models/gaze-tracking.json');
      console.log('‚úÖ Modelo de seguimiento de mirada cargado');
    } catch (error) {
      console.log('‚ö†Ô∏è Creando modelo de seguimiento de mirada...');
      
      // Modelo CNN + regresi√≥n para estimaci√≥n de punto de mirada
      const eyeRegionInput = tf.input({ shape: [60, 36, 3], name: 'eye_region_input' });
      const headPoseInput = tf.input({ shape: [3], name: 'head_pose_input' }); // pitch, yaw, roll
      const eyeGazeHistoryInput = tf.input({ shape: [5, 2], name: 'gaze_history' }); // √öltimos 5 puntos de mirada
      
      // Procesar regi√≥n ocular
      let eyeFeatures = tf.layers.conv2d({
        filters: 32,
        kernelSize: 3,
        activation: 'relu',
        name: 'gaze_conv1'
      }).apply(eyeRegionInput) as tf.SymbolicTensor;
      
      eyeFeatures = tf.layers.conv2d({
        filters: 64,
        kernelSize: 3,
        activation: 'relu',
        name: 'gaze_conv2'
      }).apply(eyeFeatures) as tf.SymbolicTensor;
      
      eyeFeatures = tf.layers.conv2d({
        filters: 128,
        kernelSize: 3,
        activation: 'relu',
        name: 'gaze_conv3'
      }).apply(eyeFeatures) as tf.SymbolicTensor;
      
      eyeFeatures = tf.layers.globalAveragePooling2d({ name: 'gaze_pool' }).apply(eyeFeatures) as tf.SymbolicTensor;
      
      // Procesar pose de la cabeza
      let headPoseFeatures = tf.layers.dense({
        units: 32,
        activation: 'relu',
        name: 'head_pose_processor'
      }).apply(headPoseInput) as tf.SymbolicTensor;
      
      // Procesar historia de mirada
      let gazeHistoryFeatures = tf.layers.lstm({
        units: 32,
        returnSequences: false,
        name: 'gaze_history_lstm'
      }).apply(eyeGazeHistoryInput) as tf.SymbolicTensor;
      
      // Combinar caracter√≠sticas
      const gazeFeatures = tf.layers.concatenate({
        name: 'gaze_feature_fusion'
      }).apply([eyeFeatures, headPoseFeatures, gazeHistoryFeatures]) as tf.SymbolicTensor;
      
      // Red de regresi√≥n para coordenadas de mirada
      let hidden = tf.layers.dense({
        units: 256,
        activation: 'relu',
        name: 'gaze_hidden1'
      }).apply(gazeFeatures) as tf.SymbolicTensor;
      
      hidden = tf.layers.dropout({ rate: 0.3 }).apply(hidden) as tf.SymbolicTensor;
      
      hidden = tf.layers.dense({
        units: 128,
        activation: 'relu',
        name: 'gaze_hidden2'
      }).apply(hidden) as tf.SymbolicTensor;
      
      // Coordenadas de mirada en la pantalla
      const gazeCoordinates = tf.layers.dense({
        units: 2,
        activation: 'sigmoid', // Normalizado a [0,1]
        name: 'gaze_coordinates'
      }).apply(hidden) as tf.SymbolicTensor;
      
      // Confianza de la predicci√≥n
      const gazeConfidence = tf.layers.dense({
        units: 1,
        activation: 'sigmoid',
        name: 'gaze_confidence'
      }).apply(hidden) as tf.SymbolicTensor;
      
      // Estimaci√≥n de fijaci√≥n vs sacada
      const gazeType = tf.layers.dense({
        units: 3, // fixation, saccade, blink
        activation: 'softmax',
        name: 'gaze_type'
      }).apply(hidden) as tf.SymbolicTensor;
      
      this.gazeTrackingModel = tf.model({
        inputs: [eyeRegionInput, headPoseInput, eyeGazeHistoryInput],
        outputs: [gazeCoordinates, gazeConfidence, gazeType],
        name: 'gaze_tracking_model'
      });
      
      this.gazeTrackingModel.compile({
        optimizer: tf.train.adam(0.001),
        loss: {
          'gaze_coordinates': 'meanSquaredError',
          'gaze_confidence': 'meanSquaredError',
          'gaze_type': 'categoricalCrossentropy'
        },
        metrics: ['mae', 'accuracy']
      });
      
      console.log('‚úÖ Modelo de seguimiento de mirada creado');
    }
  }

  /**
   * Inicializar modelo de engagement
   */
  private async initializeEngagementModel(): Promise<void> {
    console.log('‚ö†Ô∏è Creando modelo de engagement...');
    
    // Modelo multimodal que combina todas las se√±ales visuales
    const emotionInput = tf.input({ shape: [13], name: 'emotion_features' }); // 13 tipos de emociones
    const gazeInput = tf.input({ shape: [10], name: 'gaze_features' }); // Features de mirada
    const postureInput = tf.input({ shape: [8], name: 'posture_features' }); // Features de postura
    const microExpressionInput = tf.input({ shape: [6], name: 'micro_expression_features' });
    const temporalInput = tf.input({ shape: [20, 37], name: 'temporal_features' }); // 20 frames de historia
    
    // Procesar emociones
    let emotionProcessed = tf.layers.dense({
      units: 32,
      activation: 'relu',
      name: 'emotion_processor'
    }).apply(emotionInput) as tf.SymbolicTensor;
    
    // Procesar mirada
    let gazeProcessed = tf.layers.dense({
      units: 24,
      activation: 'relu',
      name: 'gaze_processor'
    }).apply(gazeInput) as tf.SymbolicTensor;
    
    // Procesar postura
    let postureProcessed = tf.layers.dense({
      units: 16,
      activation: 'relu',
      name: 'posture_processor'
    }).apply(postureInput) as tf.SymbolicTensor;
    
    // Procesar microexpresiones
    let microProcessed = tf.layers.dense({
      units: 12,
      activation: 'relu',
      name: 'micro_processor'
    }).apply(microExpressionInput) as tf.SymbolicTensor;
    
    // Procesar caracter√≠sticas temporales
    let temporalProcessed = tf.layers.lstm({
      units: 64,
      returnSequences: false,
      name: 'temporal_processor'
    }).apply(temporalInput) as tf.SymbolicTensor;
    
    // Attention mechanism entre diferentes modalidades
    const multimodalAttention = this.createMultimodalAttention([
      emotionProcessed,
      gazeProcessed,
      postureProcessed,
      microProcessed
    ]);
    
    // Combinar con informaci√≥n temporal
    const allFeatures = tf.layers.concatenate({
      name: 'engagement_feature_fusion'
    }).apply([multimodalAttention, temporalProcessed]) as tf.SymbolicTensor;
    
    // Red de engagement profunda
    let hidden = tf.layers.dense({
      units: 256,
      activation: 'relu',
      name: 'engagement_layer1'
    }).apply(allFeatures) as tf.SymbolicTensor;
    
    hidden = tf.layers.dropout({ rate: 0.3 }).apply(hidden) as tf.SymbolicTensor;
    
    hidden = tf.layers.dense({
      units: 128,
      activation: 'relu',
      name: 'engagement_layer2'
    }).apply(hidden) as tf.SymbolicTensor;
    
    hidden = tf.layers.dropout({ rate: 0.2 }).apply(hidden) as tf.SymbolicTensor;
    
    // M√∫ltiples outputs de engagement
    const overallEngagement = tf.layers.dense({
      units: 1,
      activation: 'sigmoid',
      name: 'overall_engagement'
    }).apply(hidden) as tf.SymbolicTensor;
    
    const attentionLevel = tf.layers.dense({
      units: 1,
      activation: 'sigmoid',
      name: 'attention_level'
    }).apply(hidden) as tf.SymbolicTensor;
    
    const cognitiveLoad = tf.layers.dense({
      units: 1,
      activation: 'sigmoid',
      name: 'cognitive_load'
    }).apply(hidden) as tf.SymbolicTensor;
    
    const fatigueLevel = tf.layers.dense({
      units: 1,
      activation: 'sigmoid',
      name: 'fatigue_level'
    }).apply(hidden) as tf.SymbolicTensor;
    
    const distractionProbability = tf.layers.dense({
      units: 1,
      activation: 'sigmoid',
      name: 'distraction_probability'
    }).apply(hidden) as tf.SymbolicTensor;
    
    this.engagementModel = tf.model({
      inputs: [emotionInput, gazeInput, postureInput, microExpressionInput, temporalInput],
      outputs: [overallEngagement, attentionLevel, cognitiveLoad, fatigueLevel, distractionProbability],
      name: 'engagement_analysis_model'
    });
    
    this.engagementModel.compile({
      optimizer: tf.train.adam(0.001),
      loss: 'meanSquaredError',
      metrics: ['mae']
    });
    
    console.log('‚úÖ Modelo de engagement creado');
  }

  /**
   * Inicializar an√°lisis postural
   */
  private async initializePostureAnalysis(): Promise<void> {
    console.log('‚ö†Ô∏è Creando modelo de an√°lisis postural...');
    
    // Modelo para an√°lisis de postura corporal y ergonom√≠a
    const bodyLandmarksInput = tf.input({ shape: [33, 2], name: 'body_landmarks' }); // 33 puntos corporales
    const headPoseInput = tf.input({ shape: [3], name: 'head_pose' }); // pitch, yaw, roll
    const temporalPostureInput = tf.input({ shape: [15, 8], name: 'posture_history' }); // Historia postural
    
    // Procesar landmarks corporales
    let landmarksFlat = tf.layers.flatten({ name: 'landmarks_flatten' }).apply(bodyLandmarksInput) as tf.SymbolicTensor;
    
    let landmarksProcessed = tf.layers.dense({
      units: 128,
      activation: 'relu',
      name: 'landmarks_processor'
    }).apply(landmarksFlat) as tf.SymbolicTensor;
    
    // Procesar pose de cabeza
    let headPoseProcessed = tf.layers.dense({
      units: 16,
      activation: 'relu',
      name: 'head_pose_processor'
    }).apply(headPoseInput) as tf.SymbolicTensor;
    
    // Procesar historia postural
    let postureHistoryProcessed = tf.layers.lstm({
      units: 32,
      returnSequences: false,
      name: 'posture_history_processor'
    }).apply(temporalPostureInput) as tf.SymbolicTensor;
    
    // Combinar caracter√≠sticas posturales
    const postureFeatures = tf.layers.concatenate({
      name: 'posture_feature_fusion'
    }).apply([landmarksProcessed, headPoseProcessed, postureHistoryProcessed]) as tf.SymbolicTensor;
    
    // Red de an√°lisis postural
    let hidden = tf.layers.dense({
      units: 128,
      activation: 'relu',
      name: 'posture_analysis_layer'
    }).apply(postureFeatures) as tf.SymbolicTensor;
    
    // Outputs posturales espec√≠ficos
    const shoulderAlignment = tf.layers.dense({
      units: 1,
      activation: 'sigmoid',
      name: 'shoulder_alignment'
    }).apply(hidden) as tf.SymbolicTensor;
    
    const spinalPosture = tf.layers.dense({
      units: 1,
      activation: 'sigmoid',
      name: 'spinal_posture'
    }).apply(hidden) as tf.SymbolicTensor;
    
    const headStability = tf.layers.dense({
      units: 1,
      activation: 'sigmoid',
      name: 'head_stability'
    }).apply(hidden) as tf.SymbolicTensor;
    
    const ergonomicScore = tf.layers.dense({
      units: 1,
      activation: 'sigmoid',
      name: 'ergonomic_score'
    }).apply(hidden) as tf.SymbolicTensor;
    
    const leaningDirection = tf.layers.dense({
      units: 5, // forward, backward, left, right, neutral
      activation: 'softmax',
      name: 'leaning_direction'
    }).apply(hidden) as tf.SymbolicTensor;
    
    this.postureModel = tf.model({
      inputs: [bodyLandmarksInput, headPoseInput, temporalPostureInput],
      outputs: [shoulderAlignment, spinalPosture, headStability, ergonomicScore, leaningDirection],
      name: 'posture_analysis_model'
    });
    
    this.postureModel.compile({
      optimizer: tf.train.adam(0.001),
      loss: 'meanSquaredError',
      metrics: ['mae']
    });
    
    console.log('‚úÖ Modelo de an√°lisis postural creado');
  }

  /**
   * Inicializar detecci√≥n de microexpresiones
   */
  private async initializeMicroExpressionDetection(): Promise<void> {
    console.log('‚ö†Ô∏è Creando modelo de detecci√≥n de microexpresiones...');
    
    // Modelo especializado en microexpresiones (duraciones muy cortas)
    const faceSequenceInput = tf.input({ shape: [8, 64, 64, 3], name: 'face_sequence' }); // 8 frames secuenciales
    const facialLandmarksInput = tf.input({ shape: [8, 68, 2], name: 'facial_landmarks_sequence' }); // Landmarks faciales
    
    // CNN 3D para procesar secuencia temporal facial
    let conv3d = tf.layers.conv3d({
      filters: 32,
      kernelSize: [3, 3, 3],
      activation: 'relu',
      name: 'micro_conv3d_1'
    }).apply(faceSequenceInput) as tf.SymbolicTensor;
    
    conv3d = tf.layers.conv3d({
      filters: 64,
      kernelSize: [3, 3, 3],
      activation: 'relu',
      name: 'micro_conv3d_2'
    }).apply(conv3d) as tf.SymbolicTensor;
    
    conv3d = tf.layers.maxPooling3d({
      poolSize: [2, 2, 2],
      name: 'micro_pool3d'
    }).apply(conv3d) as tf.SymbolicTensor;
    
    const spatialFeatures = tf.layers.globalAveragePooling3d({
      name: 'micro_spatial_pool'
    }).apply(conv3d) as tf.SymbolicTensor;
    
    // Procesar landmarks temporales
    const landmarksFlat = tf.layers.reshape({
      targetShape: [8, 136], // Flatten landmarks per frame
      name: 'landmarks_reshape'
    }).apply(facialLandmarksInput) as tf.SymbolicTensor;
    
    const temporalFeatures = tf.layers.lstm({
      units: 64,
      returnSequences: false,
      name: 'micro_temporal_lstm'
    }).apply(landmarksFlat) as tf.SymbolicTensor;
    
    // Combinar caracter√≠sticas espaciales y temporales
    const microFeatures = tf.layers.concatenate({
      name: 'micro_feature_fusion'
    }).apply([spatialFeatures, temporalFeatures]) as tf.SymbolicTensor;
    
    // Red de clasificaci√≥n de microexpresiones
    let hidden = tf.layers.dense({
      units: 256,
      activation: 'relu',
      name: 'micro_classifier_1'
    }).apply(microFeatures) as tf.SymbolicTensor;
    
    hidden = tf.layers.dropout({ rate: 0.4 }).apply(hidden) as tf.SymbolicTensor;
    
    hidden = tf.layers.dense({
      units: 128,
      activation: 'relu',
      name: 'micro_classifier_2'
    }).apply(hidden) as tf.SymbolicTensor;
    
    // Outputs de microexpresiones
    const microEmotions = tf.layers.dense({
      units: 13, // 13 tipos de microemociones
      activation: 'sigmoid',
      name: 'micro_emotions'
    }).apply(hidden) as tf.SymbolicTensor;
    
    const microIntensity = tf.layers.dense({
      units: 1,
      activation: 'sigmoid',
      name: 'micro_intensity'
    }).apply(hidden) as tf.SymbolicTensor;
    
    const suppressionDetection = tf.layers.dense({
      units: 1,
      activation: 'sigmoid',
      name: 'emotion_suppression'
    }).apply(hidden) as tf.SymbolicTensor;
    
    const authenticity = tf.layers.dense({
      units: 1,
      activation: 'sigmoid',
      name: 'emotion_authenticity'
    }).apply(hidden) as tf.SymbolicTensor;
    
    this.microExpressionModel = tf.model({
      inputs: [faceSequenceInput, facialLandmarksInput],
      outputs: [microEmotions, microIntensity, suppressionDetection, authenticity],
      name: 'micro_expression_model'
    });
    
    this.microExpressionModel.compile({
      optimizer: tf.train.adam(0.0005),
      loss: {
        'micro_emotions': 'binaryCrossentropy',
        'micro_intensity': 'meanSquaredError',
        'emotion_suppression': 'binaryCrossentropy',
        'emotion_authenticity': 'meanSquaredError'
      },
      metrics: ['accuracy', 'mae']
    });
    
    console.log('‚úÖ Modelo de detecci√≥n de microexpresiones creado');
  }

  /**
   * Inicializar modelo de carga cognitiva
   */
  private async initializeCognitiveLoadModel(): Promise<void> {
    console.log('‚ö†Ô∏è Creando modelo de carga cognitiva...');
    
    // Modelo basado en indicadores neur√≥fisiol√≥gicos
    const pupilDataInput = tf.input({ shape: [30, 3], name: 'pupil_data' }); // 30 mediciones: di√°metro, x, y
    const blinkDataInput = tf.input({ shape: [20], name: 'blink_pattern' }); // 20 intervalos de parpadeo
    const eyeMovementInput = tf.input({ shape: [25, 4], name: 'eye_movements' }); // Sacadas y fijaciones
    const facialTensionInput = tf.input({ shape: [12], name: 'facial_tension' }); // Tensi√≥n muscular facial
    
    // Procesar datos pupilares
    let pupilProcessed = tf.layers.lstm({
      units: 32,
      returnSequences: false,
      name: 'pupil_processor'
    }).apply(pupilDataInput) as tf.SymbolicTensor;
    
    // Procesar patrones de parpadeo
    let blinkProcessed = tf.layers.dense({
      units: 16,
      activation: 'relu',
      name: 'blink_processor'
    }).apply(blinkDataInput) as tf.SymbolicTensor;
    
    // Procesar movimientos oculares
    let eyeMovementProcessed = tf.layers.lstm({
      units: 24,
      returnSequences: false,
      name: 'eye_movement_processor'
    }).apply(eyeMovementInput) as tf.SymbolicTensor;
    
    // Procesar tensi√≥n facial
    let tensionProcessed = tf.layers.dense({
      units: 8,
      activation: 'relu',
      name: 'tension_processor'
    }).apply(facialTensionInput) as tf.SymbolicTensor;
    
    // Combinar indicadores neur√≥fisiol√≥gicos
    const cognitiveFeatures = tf.layers.concatenate({
      name: 'cognitive_feature_fusion'
    }).apply([pupilProcessed, blinkProcessed, eyeMovementProcessed, tensionProcessed]) as tf.SymbolicTensor;
    
    // Red de an√°lisis de carga cognitiva
    let hidden = tf.layers.dense({
      units: 128,
      activation: 'relu',
      name: 'cognitive_analyzer'
    }).apply(cognitiveFeatures) as tf.SymbolicTensor;
    
    hidden = tf.layers.dropout({ rate: 0.2 }).apply(hidden) as tf.SymbolicTensor;
    
    // Outputs espec√≠ficos de neur√≥fisiolog√≠a
    const cognitiveLoadLevel = tf.layers.dense({
      units: 1,
      activation: 'sigmoid',
      name: 'cognitive_load_level'
    }).apply(hidden) as tf.SymbolicTensor;
    
    const attentionalState = tf.layers.dense({
      units: 4, // sustained, selective, divided, flexible
      activation: 'softmax',
      name: 'attentional_state'
    }).apply(hidden) as tf.SymbolicTensor;
    
    const workingMemoryLoad = tf.layers.dense({
      units: 1,
      activation: 'sigmoid',
      name: 'working_memory_load'
    }).apply(hidden) as tf.SymbolicTensor;
    
    const executiveControl = tf.layers.dense({
      units: 1,
      activation: 'sigmoid',
      name: 'executive_control'
    }).apply(hidden) as tf.SymbolicTensor;
    
    const mentalFatigue = tf.layers.dense({
      units: 1,
      activation: 'sigmoid',
      name: 'mental_fatigue'
    }).apply(hidden) as tf.SymbolicTensor;
    
    this.cognitiveLoadModel = tf.model({
      inputs: [pupilDataInput, blinkDataInput, eyeMovementInput, facialTensionInput],
      outputs: [cognitiveLoadLevel, attentionalState, workingMemoryLoad, executiveControl, mentalFatigue],
      name: 'cognitive_load_model'
    });
    
    this.cognitiveLoadModel.compile({
      optimizer: tf.train.adam(0.001),
      loss: {
        'cognitive_load_level': 'meanSquaredError',
        'attentional_state': 'categoricalCrossentropy',
        'working_memory_load': 'meanSquaredError',
        'executive_control': 'meanSquaredError',
        'mental_fatigue': 'meanSquaredError'
      },
      metrics: ['mae', 'accuracy']
    });
    
    console.log('‚úÖ Modelo de carga cognitiva creado');
  }

  /**
   * Crear attention multimodal personalizada
   */
  private createMultimodalAttention(inputs: tf.SymbolicTensor[]): tf.SymbolicTensor {
    // Crear queries, keys, y values para cada modalidad
    const attentionOutputs: tf.SymbolicTensor[] = [];
    
    for (let i = 0; i < inputs.length; i++) {
      const query = tf.layers.dense({
        units: 32,
        name: `attention_query_${i}`
      }).apply(inputs[i]) as tf.SymbolicTensor;
      
      const key = tf.layers.dense({
        units: 32,
        name: `attention_key_${i}`
      }).apply(inputs[i]) as tf.SymbolicTensor;
      
      const value = tf.layers.dense({
        units: 32,
        name: `attention_value_${i}`
      }).apply(inputs[i]) as tf.SymbolicTensor;
      
      // Self-attention simplificado
      const attention = tf.layers.multiply({
        name: `attention_output_${i}`
      }).apply([query, key]) as tf.SymbolicTensor;
      
      const attended = tf.layers.multiply({
        name: `attended_value_${i}`
      }).apply([attention, value]) as tf.SymbolicTensor;
      
      attentionOutputs.push(attended);
    }
    
    // Concatenar outputs de atenci√≥n
    return tf.layers.concatenate({
      name: 'multimodal_attention_fusion'
    }).apply(attentionOutputs) as tf.SymbolicTensor;
  }

  /**
   * Cargar calibraciones culturales
   */
  private async loadCulturalCalibrations(): Promise<void> {
    // Calibraciones para diferentes contextos culturales
    const calibrations = {
      'maya': {
        eyeContactNorms: 0.3, // Menor contacto visual directo es culturalmente apropiado
        expressivenessBaseline: 0.6, // Expresividad moderada
        emotionalSuppressionTendency: 0.7, // Mayor tendencia a suprimir emociones
        postureFormality: 0.8, // Postura m√°s formal
        distractionTolerance: 0.4 // Menor tolerancia a distracciones
      },
      'nahuatl': {
        eyeContactNorms: 0.4,
        expressivenessBaseline: 0.7,
        emotionalSuppressionTendency: 0.6,
        postureFormality: 0.7,
        distractionTolerance: 0.5
      },
      'afrodescendiente': {
        eyeContactNorms: 0.7, // Mayor contacto visual
        expressivenessBaseline: 0.8, // Mayor expresividad
        emotionalSuppressionTendency: 0.4, // Menor supresi√≥n emocional
        postureFormality: 0.5, // Postura m√°s relajada
        distractionTolerance: 0.6
      },
      'general': {
        eyeContactNorms: 0.6,
        expressivenessBaseline: 0.7,
        emotionalSuppressionTendency: 0.5,
        postureFormality: 0.6,
        distractionTolerance: 0.5
      }
    };
    
    // Cargar calibraciones en el mapa
    Object.entries(calibrations).forEach(([culture, calibration]) => {
      this.culturalCalibration.set(culture, calibration);
    });
    
    console.log('‚úÖ Calibraciones culturales cargadas');
  }

  /**
   * Iniciar seguimiento visual
   */
  async startTracking(videoElement: HTMLVideoElement, canvasElement: HTMLCanvasElement, userId: string, culturalContext: string = 'general'): Promise<void> {
    if (!this.isInitialized) {
      throw new Error('Sistema de an√°lisis visual no inicializado');
    }

    try {
      console.log('üé• Iniciando seguimiento visual...');
      
      this.videoElement = videoElement;
      this.canvasElement = canvasElement;
      
      // Configurar stream de video
      this.stream = await navigator.mediaDevices.getUserMedia({
        video: {
          width: 1280,
          height: 720,
          frameRate: this.config.frameRate
        },
        audio: false
      });
      
      this.videoElement.srcObject = this.stream;
      await this.videoElement.play();
      
      // Configurar contexto cultural
      const culturalCalibration = this.culturalCalibration.get(culturalContext) || this.culturalCalibration.get('general')!;
      
      // Iniciar calibraci√≥n personal
      await this.performPersonalCalibration(userId, culturalCalibration);
      
      // Iniciar an√°lisis continuo
      this.isTracking = true;
      this.startContinuousAnalysis(userId, culturalContext);
      
      console.log('‚úÖ Seguimiento visual iniciado exitosamente');
    } catch (error) {
      console.error('‚ùå Error iniciando seguimiento visual:', error);
      throw error;
    }
  }

  /**
   * Realizar calibraci√≥n personal
   */
  private async performPersonalCalibration(userId: string, culturalCalibration: any): Promise<void> {
    console.log('üéØ Realizando calibraci√≥n personal...');
    
    const calibrationMetrics: EngagementMetrics[] = [];
    const calibrationDuration = this.config.calibrationDuration;
    const startTime = Date.now();
    
    // Recopilar m√©tricas de l√≠nea base durante la calibraci√≥n
    while (Date.now() - startTime < calibrationDuration) {
      try {
        const metrics = await this.analyzeCurrentFrame(userId, 'general');
        if (metrics) {
          calibrationMetrics.push(metrics);
        }
        
        // Esperar antes del siguiente an√°lisis
        await new Promise(resolve => setTimeout(resolve, this.config.analysisInterval));
      } catch (error) {
        console.warn('Error durante calibraci√≥n:', error);
      }
    }
    
    // Calcular l√≠nea base personal
    if (calibrationMetrics.length > 0) {
      this.baselineMetrics = this.calculateBaselineMetrics(calibrationMetrics);
      this.personalCalibration = {
        ...culturalCalibration,
        personalBaseline: this.baselineMetrics,
        calibrationDate: new Date(),
        sampleCount: calibrationMetrics.length
      };
    }
    
    console.log('‚úÖ Calibraci√≥n personal completada');
  }

  /**
   * Calcular m√©tricas de l√≠nea base
   */
  private calculateBaselineMetrics(metrics: EngagementMetrics[]): EngagementMetrics {
    const avgMetrics = {
      userId: metrics[0].userId,
      timestamp: new Date(),
      overallEngagement: this.average(metrics.map(m => m.overallEngagement)),
      attentionLevel: this.average(metrics.map(m => m.attentionLevel)),
      emotionalState: this.averageEmotionalState(metrics.map(m => m.emotionalState)),
      cognitiveLoad: this.average(metrics.map(m => m.cognitiveLoad)),
      fatigueLevel: this.average(metrics.map(m => m.fatigueLevel)),
      distractionEvents: [],
      gazePath: [],
      posturalAnalysis: this.averagePosturalMetrics(metrics.map(m => m.posturalAnalysis)),
      microExpressions: [],
      blinkPattern: this.averageBlinkAnalysis(metrics.map(m => m.blinkPattern)),
      neurophysiologicalIndicators: this.averageNeurophysiologicalMetrics(metrics.map(m => m.neurophysiologicalIndicators)),
      culturalContext: metrics[0].culturalContext
    };
    
    return avgMetrics;
  }

  /**
   * An√°lisis continuo
   */
  private startContinuousAnalysis(userId: string, culturalContext: string): void {
    const analysisLoop = async () => {
      if (!this.isTracking) return;
      
      try {
        const metrics = await this.analyzeCurrentFrame(userId, culturalContext);
        if (metrics) {
          this.currentMetrics = metrics;
          this.metricsHistory.push(metrics);
          
          // Mantener solo los √∫ltimos 1000 registros
          if (this.metricsHistory.length > 1000) {
            this.metricsHistory = this.metricsHistory.slice(-1000);
          }
          
          // Detectar eventos importantes
          await this.detectCriticalEvents(metrics);
          
          // Emitir eventos para la aplicaci√≥n
          this.emitEngagementUpdate(metrics);
        }
      } catch (error) {
        console.error('Error en an√°lisis continuo:', error);
      }
      
      // Programar pr√≥ximo an√°lisis
      setTimeout(analysisLoop, this.config.analysisInterval);
    };
    
    // Iniciar el loop
    analysisLoop();
  }

  /**
   * Analizar frame actual
   */
  private async analyzeCurrentFrame(userId: string, culturalContext: string): Promise<EngagementMetrics | null> {
    if (!this.videoElement || !this.canvasElement) return null;

    try {
      // Capturar frame actual
      const context = this.canvasElement.getContext('2d')!;
      context.drawImage(this.videoElement, 0, 0, this.canvasElement.width, this.canvasElement.height);
      const imageData = context.getImageData(0, 0, this.canvasElement.width, this.canvasElement.height);
      
      // Convertir a tensor
      const imageTensor = tf.browser.fromPixels(imageData).expandDims(0);
      
      // An√°lisis paralelo de diferentes aspectos
      const [
        emotions,
        gazeData,
        postureData,
        microExpressions,
        cognitiveData
      ] = await Promise.all([
        this.analyzeEmotions(imageTensor),
        this.analyzeGaze(imageTensor),
        this.analyzePosture(imageTensor),
        this.analyzeMicroExpressions(imageTensor),
        this.analyzeCognitiveLoad(imageTensor)
      ]);
      
      // An√°lisis de engagement combinado
      const engagementData = await this.analyzeEngagement([
        emotions,
        gazeData,
        postureData,
        microExpressions,
        this.getTemporalFeatures()
      ]);
      
      // Aplicar ajustes culturales
      const culturallyAdjustedMetrics = this.applyCulturalAdjustments(
        engagementData,
        emotions,
        gazeData,
        postureData,
        culturalContext
      );
      
      // Construir m√©tricas completas
      const metrics: EngagementMetrics = {
        userId,
        timestamp: new Date(),
        overallEngagement: culturallyAdjustedMetrics.overallEngagement,
        attentionLevel: culturallyAdjustedMetrics.attentionLevel,
        emotionalState: emotions,
        cognitiveLoad: culturallyAdjustedMetrics.cognitiveLoad,
        fatigueLevel: culturallyAdjustedMetrics.fatigueLevel,
        distractionEvents: await this.detectDistractionEvents(gazeData, postureData),
        gazePath: this.updateGazePath(gazeData),
        posturalAnalysis: postureData,
        microExpressions: microExpressions,
        blinkPattern: await this.analyzeBlinkPattern(imageTensor),
        neurophysiologicalIndicators: cognitiveData,
        culturalContext: this.getCulturalContext(culturalContext)
      };
      
      // Limpiar tensores
      imageTensor.dispose();
      
      return metrics;
      
    } catch (error) {
      console.error('Error en an√°lisis de frame:', error);
      return null;
    }
  }

  /**
   * Analizar emociones del frame actual
   */
  private async analyzeEmotions(imageTensor: tf.Tensor): Promise<EmotionalState> {
    if (!this.emotionModel) {
      return {
        primary: 'neutral',
        intensity: 0.5,
        valence: 0,
        arousal: 0.5,
        confidence: 0,
        culturallyAdjusted: false
      };
    }

    try {
      // Extraer regiones faciales (simplificado)
      const faceRegion = tf.image.resizeBilinear(imageTensor, [64, 64]);
      const eyeRegion = tf.image.resizeBilinear(imageTensor, [32, 64]); // Aproximaci√≥n
      const mouthRegion = tf.image.resizeBilinear(imageTensor, [32, 48]); // Aproximaci√≥n
      const temporalHistory = this.getEmotionHistory();
      
      // Predicci√≥n emocional
      const predictions = await this.emotionModel.predict([
        faceRegion,
        eyeRegion,
        mouthRegion,
        temporalHistory
      ]) as tf.Tensor[];
      
      const basicEmotionsData = await predictions[0].data();
      const complexEmotionsData = await predictions[1].data();
      const valenceArousalData = await predictions[2].data();
      const intensityData = await predictions[3].data();
      
      // Procesar resultados
      const basicEmotions = Array.from(basicEmotionsData);
      const complexEmotions = Array.from(complexEmotionsData);
      const valence = valenceArousalData[0];
      const arousal = valenceArousalData[1];
      const intensity = intensityData[0];
      
      // Determinar emoci√≥n primaria
      const allEmotions = [...basicEmotions, ...complexEmotions];
      const maxIndex = allEmotions.indexOf(Math.max(...allEmotions));
      
      const emotionLabels: EmotionType[] = [
        'neutral', 'happy', 'sad', 'angry', 'fearful', 'disgusted', 'surprised',
        'confused', 'frustrated', 'concentrated', 'interested', 'bored', 'excited'
      ];
      
      const primaryEmotion = emotionLabels[maxIndex] || 'neutral';
      const confidence = allEmotions[maxIndex];
      
      // Limpiar tensores
      faceRegion.dispose();
      eyeRegion.dispose();
      mouthRegion.dispose();
      temporalHistory.dispose();
      predictions.forEach(pred => pred.dispose());
      
      return {
        primary: primaryEmotion,
        intensity,
        valence,
        arousal,
        confidence,
        culturallyAdjusted: false // Se ajustar√° despu√©s
      };
      
    } catch (error) {
      console.error('Error en an√°lisis emocional:', error);
      return {
        primary: 'neutral',
        intensity: 0.5,
        valence: 0,
        arousal: 0.5,
        confidence: 0,
        culturallyAdjusted: false
      };
    }
  }

  /**
   * Analizar datos de mirada
   */
  private async analyzeGaze(imageTensor: tf.Tensor): Promise<any> {
    if (!this.gazeTrackingModel) {
      return {
        coordinates: [0.5, 0.5],
        confidence: 0,
        type: 'fixation',
        onScreen: true
      };
    }

    try {
      // Extraer regi√≥n ocular (simplificado)
      const eyeRegion = tf.image.resizeBilinear(imageTensor, [60, 36]);
      const headPose = tf.tensor2d([[0, 0, 0]], [1, 3]); // Simplificado
      const gazeHistory = this.getGazeHistory();
      
      // Predicci√≥n de mirada
      const predictions = await this.gazeTrackingModel.predict([
        eyeRegion,
        headPose,
        gazeHistory
      ]) as tf.Tensor[];
      
      const coordinatesData = await predictions[0].data();
      const confidenceData = await predictions[1].data();
      const typeData = await predictions[2].data();
      
      const coordinates = [coordinatesData[0], coordinatesData[1]];
      const confidence = confidenceData[0];
      const typeProbs = Array.from(typeData);
      const maxTypeIndex = typeProbs.indexOf(Math.max(...typeProbs));
      const gazeType = ['fixation', 'saccade', 'blink'][maxTypeIndex] || 'fixation';
      
      // Limpiar tensores
      eyeRegion.dispose();
      headPose.dispose();
      gazeHistory.dispose();
      predictions.forEach(pred => pred.dispose());
      
      return {
        coordinates,
        confidence,
        type: gazeType,
        onScreen: coordinates[0] >= 0 && coordinates[0] <= 1 && coordinates[1] >= 0 && coordinates[1] <= 1
      };
      
    } catch (error) {
      console.error('Error en an√°lisis de mirada:', error);
      return {
        coordinates: [0.5, 0.5],
        confidence: 0,
        type: 'fixation',
        onScreen: true
      };
    }
  }

  // Implementar m√©todos auxiliares restantes...
  
  private async analyzePosture(imageTensor: tf.Tensor): Promise<PosturalMetrics> {
    // Implementar an√°lisis postural
    return {
      headPose: { pitch: 0, yaw: 0, roll: 0, stability: 1 },
      shoulderAlignment: 0.8,
      spinalPosture: 0.8,
      leaningDirection: 'neutral',
      postureChanges: 0,
      ergonomicScore: 0.8
    };
  }

  private async analyzeMicroExpressions(imageTensor: tf.Tensor): Promise<MicroExpression[]> {
    // Implementar an√°lisis de microexpresiones
    return [];
  }

  private async analyzeCognitiveLoad(imageTensor: tf.Tensor): Promise<NeurophysiologicalMetrics> {
    // Implementar an√°lisis de carga cognitiva
    return {
      cognitiveLoadIndicators: {
        pupilDilation: 0.5,
        blinkRate: 0.5,
        microsaccadeRate: 0.5,
        fixationStability: 0.8
      },
      attentionalState: {
        sustainedAttention: 0.7,
        selectiveAttention: 0.8,
        dividedAttention: 0.6,
        attentionalFlexibility: 0.7
      },
      executiveFunctionIndicators: {
        inhibitoryControl: 0.7,
        workingMemoryLoad: 0.5,
        cognitiveFlexibility: 0.8,
        planningAndOrganization: 0.7
      },
      memoryProcessing: {
        encodingEfficiency: 0.8,
        retrievalSuccess: 0.7,
        consolidationQuality: 0.8
      }
    };
  }

  private async analyzeEngagement(features: any[]): Promise<any> {
    if (!this.engagementModel) {
      return {
        overallEngagement: 0.7,
        attentionLevel: 0.8,
        cognitiveLoad: 0.5,
        fatigueLevel: 0.3,
        distractionProbability: 0.2
      };
    }

    // Implementar an√°lisis de engagement con el modelo
    return {
      overallEngagement: 0.7,
      attentionLevel: 0.8,
      cognitiveLoad: 0.5,
      fatigueLevel: 0.3,
      distractionProbability: 0.2
    };
  }

  private applyCulturalAdjustments(engagementData: any, emotions: EmotionalState, gazeData: any, postureData: PosturalMetrics, culturalContext: string): any {
    const calibration = this.culturalCalibration.get(culturalContext) || this.culturalCalibration.get('general')!;
    
    // Ajustar m√©tricas basadas en normas culturales
    const adjustedEngagement = engagementData.overallEngagement * (1 + calibration.expressivenessBaseline - 0.7);
    const adjustedAttention = engagementData.attentionLevel * (1 + calibration.eyeContactNorms - 0.6);
    
    return {
      ...engagementData,
      overallEngagement: Math.min(1, Math.max(0, adjustedEngagement)),
      attentionLevel: Math.min(1, Math.max(0, adjustedAttention))
    };
  }

  // M√©todos auxiliares para features temporales
  
  private getEmotionHistory(): tf.Tensor {
    // Obtener historia emocional reciente
    const historyLength = 10;
    const emotionCount = 13;
    
    if (this.metricsHistory.length < historyLength) {
      return tf.zeros([1, historyLength, emotionCount]);
    }
    
    const recentHistory = this.metricsHistory.slice(-historyLength);
    const historyData = recentHistory.map(metrics => {
      // Convertir estado emocional a array num√©rico
      const emotionArray = new Array(emotionCount).fill(0);
      const emotionIndex = this.getEmotionIndex(metrics.emotionalState.primary);
      emotionArray[emotionIndex] = metrics.emotionalState.intensity;
      return emotionArray;
    });
    
    return tf.tensor3d([historyData], [1, historyLength, emotionCount]);
  }

  private getGazeHistory(): tf.Tensor {
    // Obtener historia de mirada reciente
    const historyLength = 5;
    
    if (this.metricsHistory.length < historyLength) {
      return tf.zeros([1, historyLength, 2]);
    }
    
    const recentHistory = this.metricsHistory.slice(-historyLength);
    const historyData = recentHistory.map(metrics => {
      if (metrics.gazePath.length > 0) {
        const lastGaze = metrics.gazePath[metrics.gazePath.length - 1];
        return [lastGaze.x, lastGaze.y];
      }
      return [0.5, 0.5]; // Centro de pantalla por defecto
    });
    
    return tf.tensor3d([historyData], [1, historyLength, 2]);
  }

  private getTemporalFeatures(): tf.Tensor {
    // Obtener features temporales generales
    const historyLength = 20;
    const featureCount = 37; // Total de features por frame
    
    if (this.metricsHistory.length < historyLength) {
      return tf.zeros([1, historyLength, featureCount]);
    }
    
    const recentHistory = this.metricsHistory.slice(-historyLength);
    const historyData = recentHistory.map(metrics => {
      return [
        metrics.overallEngagement,
        metrics.attentionLevel,
        metrics.cognitiveLoad,
        metrics.fatigueLevel,
        metrics.emotionalState.intensity,
        metrics.emotionalState.valence,
        metrics.emotionalState.arousal,
        metrics.posturalAnalysis.headPose.pitch / 90, // Normalizar
        metrics.posturalAnalysis.headPose.yaw / 90,
        metrics.posturalAnalysis.headPose.roll / 90,
        metrics.posturalAnalysis.shoulderAlignment,
        metrics.posturalAnalysis.spinalPosture,
        metrics.posturalAnalysis.ergonomicScore,
        metrics.blinkPattern.averageRate / 30, // Normalizar
        metrics.neurophysiologicalIndicators.cognitiveLoadIndicators.pupilDilation,
        metrics.neurophysiologicalIndicators.cognitiveLoadIndicators.blinkRate,
        metrics.neurophysiologicalIndicators.attentionalState.sustainedAttention,
        metrics.neurophysiologicalIndicators.attentionalState.selectiveAttention,
        ...Array.from({ length: 19 }, () => Math.random()) // Features adicionales
      ];
    });
    
    return tf.tensor3d([historyData], [1, historyLength, featureCount]);
  }

  // M√©todos auxiliares de procesamiento
  
  private average(values: number[]): number {
    if (values.length === 0) return 0;
    return values.reduce((sum, val) => sum + val, 0) / values.length;
  }

  private averageEmotionalState(states: EmotionalState[]): EmotionalState {
    if (states.length === 0) {
      return {
        primary: 'neutral',
        intensity: 0.5,
        valence: 0,
        arousal: 0.5,
        confidence: 0,
        culturallyAdjusted: false
      };
    }
    
    return {
      primary: states[0].primary, // Usar la primera
      intensity: this.average(states.map(s => s.intensity)),
      valence: this.average(states.map(s => s.valence)),
      arousal: this.average(states.map(s => s.arousal)),
      confidence: this.average(states.map(s => s.confidence)),
      culturallyAdjusted: true
    };
  }

  private averagePosturalMetrics(metrics: PosturalMetrics[]): PosturalMetrics {
    if (metrics.length === 0) {
      return {
        headPose: { pitch: 0, yaw: 0, roll: 0, stability: 1 },
        shoulderAlignment: 0.8,
        spinalPosture: 0.8,
        leaningDirection: 'neutral',
        postureChanges: 0,
        ergonomicScore: 0.8
      };
    }
    
    return {
      headPose: {
        pitch: this.average(metrics.map(m => m.headPose.pitch)),
        yaw: this.average(metrics.map(m => m.headPose.yaw)),
        roll: this.average(metrics.map(m => m.headPose.roll)),
        stability: this.average(metrics.map(m => m.headPose.stability))
      },
      shoulderAlignment: this.average(metrics.map(m => m.shoulderAlignment)),
      spinalPosture: this.average(metrics.map(m => m.spinalPosture)),
      leaningDirection: metrics[0].leaningDirection,
      postureChanges: this.average(metrics.map(m => m.postureChanges)),
      ergonomicScore: this.average(metrics.map(m => m.ergonomicScore))
    };
  }

  private averageBlinkAnalysis(analyses: BlinkAnalysis[]): BlinkAnalysis {
    if (analyses.length === 0) {
      return {
        averageRate: 15,
        variability: 2,
        microSleepEvents: 0,
        concentrationBlinks: 0,
        stressIndicators: 0
      };
    }
    
    return {
      averageRate: this.average(analyses.map(a => a.averageRate)),
      variability: this.average(analyses.map(a => a.variability)),
      microSleepEvents: this.average(analyses.map(a => a.microSleepEvents)),
      concentrationBlinks: this.average(analyses.map(a => a.concentrationBlinks)),
      stressIndicators: this.average(analyses.map(a => a.stressIndicators))
    };
  }

  private averageNeurophysiologicalMetrics(metrics: NeurophysiologicalMetrics[]): NeurophysiologicalMetrics {
    if (metrics.length === 0) {
      return {
        cognitiveLoadIndicators: {
          pupilDilation: 0.5,
          blinkRate: 0.5,
          microsaccadeRate: 0.5,
          fixationStability: 0.8
        },
        attentionalState: {
          sustainedAttention: 0.7,
          selectiveAttention: 0.8,
          dividedAttention: 0.6,
          attentionalFlexibility: 0.7
        },
        executiveFunctionIndicators: {
          inhibitoryControl: 0.7,
          workingMemoryLoad: 0.5,
          cognitiveFlexibility: 0.8,
          planningAndOrganization: 0.7
        },
        memoryProcessing: {
          encodingEfficiency: 0.8,
          retrievalSuccess: 0.7,
          consolidationQuality: 0.8
        }
      };
    }
    
    return {
      cognitiveLoadIndicators: {
        pupilDilation: this.average(metrics.map(m => m.cognitiveLoadIndicators.pupilDilation)),
        blinkRate: this.average(metrics.map(m => m.cognitiveLoadIndicators.blinkRate)),
        microsaccadeRate: this.average(metrics.map(m => m.cognitiveLoadIndicators.microsaccadeRate)),
        fixationStability: this.average(metrics.map(m => m.cognitiveLoadIndicators.fixationStability))
      },
      attentionalState: {
        sustainedAttention: this.average(metrics.map(m => m.attentionalState.sustainedAttention)),
        selectiveAttention: this.average(metrics.map(m => m.attentionalState.selectiveAttention)),
        dividedAttention: this.average(metrics.map(m => m.attentionalState.dividedAttention)),
        attentionalFlexibility: this.average(metrics.map(m => m.attentionalState.attentionalFlexibility))
      },
      executiveFunctionIndicators: {
        inhibitoryControl: this.average(metrics.map(m => m.executiveFunctionIndicators.inhibitoryControl)),
        workingMemoryLoad: this.average(metrics.map(m => m.executiveFunctionIndicators.workingMemoryLoad)),
        cognitiveFlexibility: this.average(metrics.map(m => m.executiveFunctionIndicators.cognitiveFlexibility)),
        planningAndOrganization: this.average(metrics.map(m => m.executiveFunctionIndicators.planningAndOrganization))
      },
      memoryProcessing: {
        encodingEfficiency: this.average(metrics.map(m => m.memoryProcessing.encodingEfficiency)),
        retrievalSuccess: this.average(metrics.map(m => m.memoryProcessing.retrievalSuccess)),
        consolidationQuality: this.average(metrics.map(m => m.memoryProcessing.consolidationQuality))
      }
    };
  }

  private getEmotionIndex(emotion: EmotionType): number {
    const emotions: EmotionType[] = [
      'neutral', 'happy', 'sad', 'angry', 'fearful', 'disgusted', 'surprised',
      'confused', 'frustrated', 'concentrated', 'interested', 'bored', 'excited'
    ];
    return emotions.indexOf(emotion) || 0;
  }

  private async detectDistractionEvents(gazeData: any, postureData: PosturalMetrics): Promise<DistractionEvent[]> {
    const events: DistractionEvent[] = [];
    
    // Detectar distracciones basadas en mirada
    if (!gazeData.onScreen) {
      events.push({
        timestamp: new Date(),
        type: 'gaze_away',
        duration: 1000, // ms (estimado)
        severity: 'medium',
        impact: 0.6,
        recovery_time: 500
      });
    }
    
    // Detectar cambios posturales significativos
    if (postureData.postureChanges > 2) {
      events.push({
        timestamp: new Date(),
        type: 'posture_change',
        duration: 2000,
        severity: 'low',
        impact: 0.3,
        recovery_time: 1000
      });
    }
    
    return events;
  }

  private updateGazePath(gazeData: any): GazePoint[] {
    const newPoint: GazePoint = {
      x: gazeData.coordinates[0],
      y: gazeData.coordinates[1],
      timestamp: Date.now(),
      confidence: gazeData.confidence
    };
    
    // Mantener solo los √∫ltimos 100 puntos de mirada
    const currentPath = this.currentMetrics?.gazePath || [];
    const updatedPath = [...currentPath, newPoint];
    
    return updatedPath.length > 100 ? updatedPath.slice(-100) : updatedPath;
  }

  private async analyzeBlinkPattern(imageTensor: tf.Tensor): Promise<BlinkAnalysis> {
    // Implementar an√°lisis de patrones de parpadeo
    return {
      averageRate: 15 + Math.random() * 5, // 15-20 blinks per minute
      variability: Math.random() * 3,
      microSleepEvents: Math.random() < 0.1 ? 1 : 0,
      concentrationBlinks: Math.random() < 0.3 ? 1 : 0,
      stressIndicators: Math.random() < 0.2 ? 1 : 0
    };
  }

  private getCulturalContext(culturalContext: string): CulturalEngagementContext {
    const calibration = this.culturalCalibration.get(culturalContext) || this.culturalCalibration.get('general')!;
    
    return {
      culturalBackground: culturalContext,
      nonverbalCommunicationStyle: calibration.expressivenessBaseline > 0.7 ? 'direct' : 'indirect',
      eyeContactNorms: calibration.eyeContactNorms > 0.6 ? 'high' : calibration.eyeContactNorms > 0.4 ? 'moderate' : 'low',
      expressivenessCultural: calibration.expressivenessBaseline,
      collectivistVsIndividualist: culturalContext === 'maya' || culturalContext === 'nahuatl' ? 0.2 : 0.7,
      powerDistanceOrientation: calibration.postureFormality,
      uncertaintyAvoidance: calibration.distractionTolerance,
      culturalBiasAdjustment: 1.0 - calibration.emotionalSuppressionTendency
    };
  }

  private async detectCriticalEvents(metrics: EngagementMetrics): Promise<void> {
    // Detectar eventos cr√≠ticos que requieren intervenci√≥n inmediata
    
    if (metrics.fatigueLevel > 0.8) {
      this.emitCriticalEvent('high_fatigue', {
        severity: 'high',
        recommendation: 'Sugerir descanso inmediato',
        metrics
      });
    }
    
    if (metrics.attentionLevel < 0.3) {
      this.emitCriticalEvent('low_attention', {
        severity: 'medium',
        recommendation: 'Ajustar contenido o implementar actividad de re-engagement',
        metrics
      });
    }
    
    if (metrics.cognitiveLoad > 0.9) {
      this.emitCriticalEvent('cognitive_overload', {
        severity: 'high',
        recommendation: 'Reducir complejidad del contenido',
        metrics
      });
    }
  }

  private emitEngagementUpdate(metrics: EngagementMetrics): void {
    // Emitir evento para que la aplicaci√≥n pueda reaccionar
    if (typeof window !== 'undefined') {
      window.dispatchEvent(new CustomEvent('engagementUpdate', {
        detail: metrics
      }));
    }
  }

  private emitCriticalEvent(eventType: string, data: any): void {
    // Emitir evento cr√≠tico
    if (typeof window !== 'undefined') {
      window.dispatchEvent(new CustomEvent('criticalEngagementEvent', {
        detail: { eventType, data }
      }));
    }
  }

  /**
   * Generar heatmap de atenci√≥n
   */
  async generateAttentionHeatmap(duration: number = 60000): Promise<AttentionHeatmap> {
    const recentGazePoints = this.metricsHistory
      .filter(m => Date.now() - m.timestamp.getTime() < duration)
      .flatMap(m => m.gazePath);
    
    if (recentGazePoints.length === 0) {
      return {
        width: 1280,
        height: 720,
        data: Array(720).fill(0).map(() => Array(1280).fill(0)),
        timestamp: new Date(),
        duration,
        screenElements: []
      };
    }
    
    // Crear matriz de densidad
    const width = 1280;
    const height = 720;
    const heatmapData = Array(height).fill(0).map(() => Array(width).fill(0));
    
    // Aplicar cada punto de mirada con blur gaussiano
    recentGazePoints.forEach(point => {
      const x = Math.floor(point.x * width);
      const y = Math.floor(point.y * height);
      const radius = 50; // Radio de influencia
      
      for (let dy = -radius; dy <= radius; dy++) {
        for (let dx = -radius; dx <= radius; dx++) {
          const px = x + dx;
          const py = y + dy;
          
          if (px >= 0 && px < width && py >= 0 && py < height) {
            const distance = Math.sqrt(dx * dx + dy * dy);
            const intensity = Math.exp(-(distance * distance) / (2 * 20 * 20)) * point.confidence;
            heatmapData[py][px] += intensity;
          }
        }
      }
    });
    
    return {
      width,
      height,
      data: heatmapData,
      timestamp: new Date(),
      duration,
      screenElements: [] // Implementar detecci√≥n de elementos de pantalla
    };
  }

  /**
   * Obtener m√©tricas actuales
   */
  getCurrentMetrics(): EngagementMetrics | null {
    return this.currentMetrics;
  }

  /**
   * Obtener historia de m√©tricas
   */
  getMetricsHistory(): EngagementMetrics[] {
    return [...this.metricsHistory];
  }

  /**
   * Detener seguimiento
   */
  async stopTracking(): Promise<void> {
    this.isTracking = false;
    
    if (this.stream) {
      this.stream.getTracks().forEach(track => track.stop());
      this.stream = null;
    }
    
    this.videoElement = null;
    this.canvasElement = null;
    this.currentMetrics = null;
    
    console.log('‚úÖ Seguimiento visual detenido');
  }

  /**
   * Obtener estad√≠sticas del servicio
   */
  getServiceStats(): any {
    return {
      isInitialized: this.isInitialized,
      isTracking: this.isTracking,
      modelsLoaded: {
        faceDetection: !!this.faceDetectionModel,
        emotion: !!this.emotionModel,
        gazeTracking: !!this.gazeTrackingModel,
        engagement: !!this.engagementModel,
        posture: !!this.postureModel,
        microExpression: !!this.microExpressionModel,
        cognitiveLoad: !!this.cognitiveLoadModel
      },
      calibrations: {
        cultural: this.culturalCalibration.size,
        personal: !!this.personalCalibration,
        baseline: !!this.baselineMetrics
      },
      metricsHistory: this.metricsHistory.length,
      memoryUsage: tf.memory()
    };
  }

  /**
   * Limpiar recursos
   */
  async dispose(): Promise<void> {
    console.log('üßπ Limpiando recursos de an√°lisis visual...');

    // Detener seguimiento
    await this.stopTracking();

    // Limpiar modelos
    this.emotionModel?.dispose();
    this.gazeTrackingModel?.dispose();
    this.engagementModel?.dispose();
    this.postureModel?.dispose();
    this.microExpressionModel?.dispose();
    this.cognitiveLoadModel?.dispose();

    // Limpiar datos
    this.metricsHistory = [];
    this.culturalCalibration.clear();
    this.personalCalibration = null;
    this.baselineMetrics = null;

    this.isInitialized = false;
    console.log('‚úÖ Recursos de an√°lisis visual limpiados');
  }
}

// Instancia singleton del servicio de an√°lisis visual
export const visionAnalyticsService = new VisionAnalyticsService();